.. # Copyright (C) 2020-2022 Intel Corporation
.. # SPDX-License-Identifier: Apache-2.0

.. _flow_interface:

***************
Flow Interface
***************

What is it?
===========

A new OpenFL interface that gives significantly more flexility to researchers in the construction of federated learning experiments. It is heavily influenced by the interface and design of Metaflow, the popular framework for data scientists originally developed at Netflix. There are several reasons we converged on Metaflow as inspiration for our work:

1. Clean expression of task sequence. Flows start with a `start` task, and end with `end`. The next task in the sequence is called by `self.next`.
2. Easy selection of what should be sent between tasks using `include` or `exclude`
3. Excellent tooling ecosystem: the metaflow client gives easy access to prior runs, tasks, and data artifacts generated by an experiment. 

There are several modifications we make in our reimagined version of this interface that are necessary for federated learning:

1. *Placement*: Metaflow's `@step` decorator is replaced by placement decorators that specify where a task will run. In horizontal federated learning, there are server (or aggregator) and client (or collaborator) nodes. Tasks decorated by `@aggregator` will run on the aggregator node, and `@collaborator` will run on the collaborator node. These placement decorators are interpreted by *Runtime* implementations: these do the heavy lifting of figuring out how to get the state of the current task to another process or node. 
2. *Runtime*: Each flow has a `.runtime` attribute. The runtime encapsulates the details of the infrastucture where the flow will run. In this experimental release, we support only a `LocalRuntime` single node implementation, but as this work matures, we will extend to a `FederatedRuntime` that implements distributed operation across remote infrastructure.
3. *Conditional branches*: 
4. *Loops*: Internal loops are within a flow; this is necessary to support rounds of training where the same sequence of tasks is performed repeatedly.   

Background
==========

Prior interfaces in OpenFL support the standard horizontal FL training workflow:

    1. The collaborator downloads the latest model from the aggregator
    2. The collaborator performs validation with their local validation dataset on the aggregated model, and sends these metrics to the aggregator (aggregated_model_validation task)
    3. The collaborator trains the model on their local training data set, and sends the local model weights and metrics to the aggregator (train task)
    4. The collaborator performs validation with their local validation dataset on their locally trained model, and sends their validation metrics to the aggregator (locally_tuned_model_validation task)
    5. The aggregator applies an aggregation function (weighted average, FedCurv, FedProx, etc.) to the model weights, and reports the aggregate metrics.

The Task Assigner determines the list of collaborator tasks to be performed, and both in the task runner API as well as the interactive API these tasks can be modified (to varying degrees). For example, to perform federated evaluation of a model, only the aggregated_model_validation task would be selected for the assigner's block of the federated plan. Equivalently for the interactive API, this can be done by only registering a single validation task. But there are many other types of workflows that can't be easily represented purely by training / validation tasks performed on a collaborator with a single model. An example is training a Federated Generative Adversarial Network (GAN); because this may be represented by separate generative and discriminator models, and could leak information about a collaborator dataset, the interface we provide should allow for better control over what gets sent over the network and how. Another common request we get is for validation with an aggregator's dataset after training. Prior to |productName| 1.5, there has not a great way to support this in OpenFL.

Goals
=====

    1. Simplify the federated workflow representation
    2. Clean separation of workflow from runtime infrastructure
    3. Help users better understand the steps in federated learning (weight extraction, tensor compression, etc.)
    4. Interface makes it clear what is sent across the network
    5. The placement of tasks and how they connect should be straightforward
    6. Don't reinvent unless absolutely necessary

Design
======

In the design of the flow interface, we evaluated many existing pipeline frameworks and workflow platforms in the python ecosystem. Airflow, Luigi, Flyte and others.   

Workflow Interface API
======================

The workflow interface formulates the experiment as a series of tasks, or a flow. Every flow begins with the `start` task and concludes with `end`.

WIP
