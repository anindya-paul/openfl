{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precise-studio",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.runtime import LocalRuntime\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "def FedAvg(models):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = np.sum([state[key] for state in state_dicts],axis=0) / len(models)\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model\n",
    "\n",
    "def inference(network,test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator step \"start\" registered\n",
      "Collaborator step \"aggregated_model_validation\" registered\n",
      "Collaborator step \"train\" registered\n",
      "Collaborator step \"local_model_validation\" registered\n",
      "Aggregator step \"join\" registered\n",
      "Aggregator step \"end\" registered\n"
     ]
    }
   ],
   "source": [
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model = None, optimizer = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start step.\n",
    "        \"\"\"\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.next(self.aggregated_model_validation,foreach='collaborators',exclude=['private'])\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        \"\"\"\n",
    "        Perform Aggregated model validation.\n",
    "        \"\"\"\n",
    "        print(f'Performing aggregated model validation for collaborator {self.input}')\n",
    "        self.agg_validation_score = inference(self.model,self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Local Model Training.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "          self.optimizer.zero_grad()\n",
    "          output = self.model(data)\n",
    "          loss = F.nll_loss(output, target)\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "          if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "               batch_idx * len(data), len(self.train_loader.dataset),\n",
    "              100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "            self.loss = loss.item()\n",
    "            torch.save(self.model.state_dict(), 'model.pth')\n",
    "            torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        \"\"\"\n",
    "        Local Model Validation.\n",
    "        \"\"\"\n",
    "        self.local_validation_score = inference(self.model,self.test_loader)\n",
    "        print(f'Doing local model validation for collaborator {self.input}: {self.local_validation_score}')\n",
    "        self.next(self.join, exclude=['training_completed'])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self,inputs):\n",
    "        \"\"\"\n",
    "        Model Aggregation.\n",
    "        \"\"\"\n",
    "        self.average_loss = sum(input.loss for input in inputs)/len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(input.agg_validation_score for input in inputs)/len(inputs)\n",
    "        self.local_model_accuracy = sum(input.local_validation_score for input in inputs)/len(inputs)\n",
    "        print(f'Average aggregated model validation values = {self.aggregated_model_accuracy}')\n",
    "        print(f'Average training loss = {self.average_loss}')\n",
    "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End step.\n",
    "        \"\"\"\n",
    "        print(f'This is the end of the flow')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forward-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 21:17:35,105\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local runtime collaborators = {'Portland': <openfl.experimental.interface.participants.Collaborator object at 0x7f811c5ebdf0>, 'Seattle': <openfl.experimental.interface.participants.Collaborator object at 0x7f811c5ebd00>, 'Chandler': <openfl.experimental.interface.participants.Collaborator object at 0x7f811c5ebe50>, 'Bangalore': <openfl.experimental.interface.participants.Collaborator object at 0x7f811c5ebee0>}\n",
      "Starting round 0...\n",
      "Created flow FederatedFlow\n",
      "\n",
      "Calling start\n",
      "Performing initialization for model\n",
      "Saving data artifacts for start\n",
      "Saved data artifacts for start\n",
      "Sending state from aggregator to collaborators\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Performing aggregated model validation for collaborator Portland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Test set: Avg. loss: 2.3264, Accuracy: 309/2500 (12%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Portland value of 0.12359999865293503\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Saving data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Performing aggregated model validation for collaborator Seattle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Test set: Avg. loss: 2.3319, Accuracy: 272/2500 (11%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Seattle value of 0.1088000014424324\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Performing aggregated model validation for collaborator Chandler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Saving data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Calling train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.322124\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.313933\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Test set: Avg. loss: 2.3338, Accuracy: 284/2500 (11%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Chandler value of 0.1136000007390976\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.294296\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Saving data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.293537\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.253793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.367113\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.239923\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.224169\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.329382\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.167799\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.311231\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.111420\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.275954\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.322638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.106058\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.259324\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 2.000380\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.308044\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.238619\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.292286\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 1.978519\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.134266\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.779590\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.249674\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.196947\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 1.727687\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.249871\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.030365\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 1.604104\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 1.984230\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.279357\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.473875\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 1.820137\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.215947\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.345559\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.173234\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 1.783607\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.552310\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.074744\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.756191\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.093956\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.197804\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 1.502963\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.252703\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 1.877183\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 1.502996\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.420492\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 1.811035\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.354180\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.993331\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.849405\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.203092\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 1.078141\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.175520\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 1.802979\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 1.164472\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 1.484607\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.021596\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.527901\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.925586\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Calling local_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.402102\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.057578\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.573820\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.948851\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.426319\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Test set: Avg. loss: 0.6865, Accuracy: 2004/2500 (80%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Doing local model validation for collaborator Portland: 0.8015999794006348\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.913160\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.458991\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.931919\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452531)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.224930\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 1.257801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 1.079180\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 1.096061\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Test set: Avg. loss: 0.6087, Accuracy: 2077/2500 (83%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Doing local model validation for collaborator Seattle: 0.8307999968528748\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Saving data artifacts for local_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452532)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Test set: Avg. loss: 0.7447, Accuracy: 1975/2500 (79%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Doing local model validation for collaborator Chandler: 0.7900000214576721\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452529)\u001b[0m Should transfer from local_model_validation to join\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2116 MiB, 8 objects, write throughput 213 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Performing aggregated model validation for collaborator Bangalore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Test set: Avg. loss: 2.3345, Accuracy: 272/2500 (11%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Bangalore value of 0.1088000014424324\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Saving data artifacts for aggregated_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.350772\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.323587\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.273255\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.244991\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.264202\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.223258\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.208753\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.140794\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.067129\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.004644\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 1.851577\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 1.776913\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.625227\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 1.548539\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 1.614248\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.273232\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.082509\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.000034\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.133828\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.164412\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.060656\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 1.100911\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.800130\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.757004\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Saving data artifacts for train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Test set: Avg. loss: 0.6157, Accuracy: 2102/2500 (84%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Doing local model validation for collaborator Bangalore: 0.8407999873161316\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=452873)\u001b[0m Should transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "Average aggregated model validation values = 0.11370000056922436\n",
      "Average training loss = 0.987363874912262\n",
      "Average local model validation values = 0.8157999962568283\n",
      "Saving data artifacts for join\n",
      "Saved data artifacts for join\n",
      "\n",
      "Calling end\n",
      "This is the end of the flow\n",
      "Saving data artifacts for end\n",
      "Saved data artifacts for end\n",
      "Accuracy improved to 0.11370000056922436 for round 0\n",
      "Starting round 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created flow FederatedFlow\n",
      "\n",
      "Calling start\n",
      "Performing initialization for model\n",
      "Saving data artifacts for start\n",
      "Saved data artifacts for start\n",
      "Sending state from aggregator to collaborators\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Performing aggregated model validation for collaborator Portland\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Performing aggregated model validation for collaborator Seattle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Test set: Avg. loss: 0.6670, Accuracy: 2098/2500 (84%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Portland value of 0.8392000198364258\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Test set: Avg. loss: 0.6717, Accuracy: 2114/2500 (85%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Seattle value of 0.8456000089645386\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Saving data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Saving data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Performing aggregated model validation for collaborator Chandler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Test set: Avg. loss: 0.6791, Accuracy: 2093/2500 (84%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Chandler value of 0.8371999859809875\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Calling train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.139104\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.951281\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Saving data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 1.041309\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.884187\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 1.348441\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.839536\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 1.201610\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.946094\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.991957\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.983026\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 1.098146\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.740536\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Calling train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.245224\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 1.011861\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.710436\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.958970\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.959797\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.538812\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.922435\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.792421\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.552893\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.907020\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.710756\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.778611\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.103794\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.695968\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 1.063214\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 1.331548\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.635115\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.679492\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 1.003535\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.967673\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.566097\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.751444\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.746440\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.694389\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.854043\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.958754\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.635220\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.915817\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.837810\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.572224\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.846116\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.655553\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.506566\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.698535\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.772813\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.553791\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.648263\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.611777\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.648732\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.681672\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.057844\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.580768\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.654162\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.570935\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.493910\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.615153\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.442823\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.589131\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.757676\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.806538\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.509336\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.521121\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.725124\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.555513\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.762183\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.889774\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Calling local_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.707175\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.866421\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Test set: Avg. loss: 0.3601, Accuracy: 2245/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Doing local model validation for collaborator Portland: 0.8980000019073486\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.897560\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Test set: Avg. loss: 0.3554, Accuracy: 2244/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Doing local model validation for collaborator Seattle: 0.897599995136261\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.897626\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453007)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453006)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Calling local_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 4233 MiB, 16 objects, write throughput 242 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Test set: Avg. loss: 0.3535, Accuracy: 2253/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Doing local model validation for collaborator Chandler: 0.901199996471405\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453058)\u001b[0m Should transfer from local_model_validation to join\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Performing aggregated model validation for collaborator Bangalore\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Test set: Avg. loss: 0.6620, Accuracy: 2119/2500 (85%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Bangalore value of 0.847599983215332\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Saving data artifacts for aggregated_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m /tmp/ipykernel_452398/1561403287.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Saved data artifacts for aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.024345\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.919930\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.893423\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 1.176813\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.864786\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 1.004577\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.687753\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.753325\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.621512\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.728890\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.529832\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.600307\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.789371\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.642435\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.507074\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.576987\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.596503\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.586696\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.662549\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.525529\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.455784\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.436438\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.566496\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.616761\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Saving data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Saved data artifacts for train\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Calling local_model_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m /home/scngupta/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Test set: Avg. loss: 0.3556, Accuracy: 2240/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Doing local model validation for collaborator Bangalore: 0.8960000276565552\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Saving data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Saved data artifacts for local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=453277)\u001b[0m Should transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "Average aggregated model validation values = 0.842399999499321\n",
      "Average training loss = 0.6987558454275131\n",
      "Average local model validation values = 0.8982000052928925\n",
      "Saving data artifacts for join\n",
      "Saved data artifacts for join\n",
      "\n",
      "Calling end\n",
      "This is the end of the flow\n",
      "Saving data artifacts for end\n",
      "Saved data artifacts for end\n",
      "Accuracy improved to 0.842399999499321 for round 1\n"
     ]
    }
   ],
   "source": [
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::len(collaborators)]\n",
    "    local_train.targets = mnist_train.targets[idx::len(collaborators)]\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators)\n",
    "print(f'Local runtime collaborators = {local_runtime._collaborators}')\n",
    "\n",
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "top_model_accuracy = 0\n",
    "for i in range(2):\n",
    "    print(f'Starting round {i}...')\n",
    "    flflow = FederatedFlow(model,optimizer,checkpoint=True)\n",
    "    flflow.runtime = local_runtime\n",
    "    flflow.run()\n",
    "    model = flflow.model\n",
    "    optimizer = flflow.optimizer\n",
    "    aggregated_model_accuracy = flflow.aggregated_model_accuracy\n",
    "    if aggregated_model_accuracy > top_model_accuracy:\n",
    "        print(f'Accuracy improved to {aggregated_model_accuracy} for round {i}')\n",
    "        top_model_accuracy = aggregated_model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddea757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.experimental.utilities.ui import InspectFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c82ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flowgraph generated at :/home/scngupta/.metaflow/FederatedFlow/1670255320094610\n"
     ]
    }
   ],
   "source": [
    "if flflow._checkpoint:\n",
    "    InspectFlow(flflow, flflow._run_id, show_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verified-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = flflow._run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composed-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statutory-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Metaflow, Flow, Task, Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifty-tamil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flow('FederatedMnistFlowWithWatermarking'),\n",
       " Flow('FederatedFlow_MNIST_Watermarking'),\n",
       " Flow('FederatedFlow')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Metaflow()\n",
    "list(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grand-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Flow('FederatedFlow').latest_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "incident-novelty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run('FederatedFlow/1670255320094610')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increasing-dressing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Step('FederatedFlow/1670255320094610/end'),\n",
       " Step('FederatedFlow/1670255320094610/join'),\n",
       " Step('FederatedFlow/1670255320094610/local_model_validation'),\n",
       " Step('FederatedFlow/1670255320094610/train'),\n",
       " Step('FederatedFlow/1670255320094610/aggregated_model_validation'),\n",
       " Step('FederatedFlow/1670255320094610/start')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "olympic-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Step(f'FederatedFlow/{run_id}/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "awful-posting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step('FederatedFlow/1670255320094610/train')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "median-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task('FederatedFlow/1670255320094610/train/1')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adult-maldives",
   "metadata": {},
   "outputs": [
    {
     "ename": "MetaflowNotFound",
     "evalue": "Task('FederatedFlow/1670255320094610/train/7') does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMetaflowNotFound\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mTask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFederatedFlow/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train/7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/metaflow/client/core.py:1033\u001b[0m, in \u001b[0;36mTask.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/metaflow/client/core.py:373\u001b[0m, in \u001b[0;36mMetaflowObject.__init__\u001b[0;34m(self, pathspec, attempt, _object, _parent, _namespace_check)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pathspec \u001b[38;5;241m=\u001b[39m pathspec\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;241m=\u001b[39m _object\n",
      "File \u001b[0;32m~/miniconda3/envs/env_openfl_github_scngupta_watermarking/lib/python3.8/site-packages/metaflow/client/core.py:405\u001b[0m, in \u001b[0;36mMetaflowObject._get_object\u001b[0;34m(self, *path_components)\u001b[0m\n\u001b[1;32m    401\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metaflow\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_object(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attempt, \u001b[38;5;241m*\u001b[39mpath_components\n\u001b[1;32m    403\u001b[0m )\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MetaflowNotFound(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mMetaflowNotFound\u001b[0m: Task('FederatedFlow/1670255320094610/train/7') does not exist"
     ]
    }
   ],
   "source": [
    "t = Task(f'FederatedFlow/{run_id}/train/7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-working",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f7fdd4a1f621cec7a25690f8e21ae706f009fca3d4d2c40a83d34a2d1ea6938"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
