{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# Workflow Interface 201: Using Ray to request exclusive GPUs\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/psfoley/openfl/blob/experimental-workflow-interface/openfl-tutorials/experimental/Workflow_Interface_201_Exclusive_GPUs_with_Ray.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd059520",
   "metadata": {},
   "source": [
    "In this OpenFL Workflow Interface tutorial, we will demonstrate how to leverage multiple GPUs concurrent model training using the LocalRuntime's Ray Backend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb89b6",
   "metadata": {},
   "source": [
    "First we start by installing the necessary dependencies for the workflow interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f98600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openfl\n",
    "!pip install metaflow\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237eac4",
   "metadata": {},
   "source": [
    "We begin with the quintessential example of a small pytorch CNN model trained on the MNIST dataset. Let's start define our dataloaders, model, optimizer, and some helper functions like we would for any other deep learning experiment. Notice the `inference` functions makes consistent references to `cuda:0`. This will become important later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e85e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "def inference(network,test_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        network = network.to('cuda:0')\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "          data = data.to('cuda:0')\n",
    "          target = target.to('cuda:0')\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, `LocalRuntime`, and placement decorators.\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `Runtime` – Defines where the flow runs, infrastructure for task transitions (how information gets sent). The `LocalRuntime` runs the flow on a single node.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "precise-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.runtime import LocalRuntime\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "\n",
    "def FedAvg(models):\n",
    "    models = [model.to('cpu') for model in models]\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = np.sum([state[key]\n",
    "                                 for state in state_dicts], axis=0) / len(models)\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2e45614",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we come to the updated flow definition. Here we request `@collaborator(num_gpus=1)` as the placement decorator, which will require a dedicated GPU for each collaborator task. Tune this based on your use case, but because this uses Ray internally, you can also pass through a [fraction of a GPU](https://docs.ray.io/en/latest/ray-core/tasks/using-ray-with-gpus.html#fractional-gpus), which will allow more than one task to run on each GPU (i.e. `@collaborator(num_gpus=0.5)` would result in two tasks per GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator step \"start\" registered\n",
      "Collaborator step \"aggregated_model_validation\" registered\n",
      "Collaborator step \"train\" registered\n",
      "Collaborator step \"local_model_validation\" registered\n",
      "Aggregator step \"join\" registered\n",
      "Aggregator step \"end\" registered\n"
     ]
    }
   ],
   "source": [
    "class CollaboratorGPUFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model = None, optimizer = None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "        self.rounds = rounds\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(self.aggregated_model_validation,foreach='collaborators',exclude=['private'])\n",
    "\n",
    "    @collaborator(num_gpus=1)\n",
    "    def aggregated_model_validation(self):\n",
    "        print(f'Performing aggregated model validation for collaborator {self.input}')\n",
    "        self.agg_validation_score = inference(self.model,self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator(num_gpus=1)\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                              momentum=momentum)\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.to(\"cuda:0\")\n",
    "                target = target.to(\"cuda:0\")\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "                self.loss = loss.item()\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator(num_gpus=1)\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model,self.test_loader)\n",
    "        print(f'Doing local model validation for collaborator {self.input}: {self.local_validation_score}')\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self,inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs)/len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(input.agg_validation_score for input in inputs)/len(inputs)\n",
    "        self.local_model_accuracy = sum(input.local_validation_score for input in inputs)/len(inputs)\n",
    "        print(f'Average aggregated model validation values = {self.aggregated_model_accuracy}')\n",
    "        print(f'Average training loss = {self.average_loss}')\n",
    "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(self.aggregated_model_validation, foreach='collaborators', exclude=['private'])\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "        \n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'This is the end of the flow')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a133f9f",
   "metadata": {},
   "source": [
    "You'll notice in the `FederatedFlow` definition above that there were certain attributes that the flow was not initialized with, namely the `train_loader` and `test_loader` for each of the collaborators. These are **private_attributes** that are exposed only throught he runtime. Each participant has it's own set of private attributes: a dictionary where the key is the attribute name, and the value is the object that will be made accessible through that participant's task. \n",
    "\n",
    "Below, we segment shards of the MNIST dataset for **four collaborators**: Portland, Seattle, Chandler, and Portland. Each has their own slice of the dataset that's accessible via the `train_loader` or `test_loader` attribute. Note that the private attributes are flexible, and you can choose to pass in a completely different type of object to any of the collaborators or aggregator (with an arbitrary name). These private attributes will always be filtered out of the current state when transfering from collaborator to aggregator, or vice versa. \n",
    "\n",
    "The LocalRuntime is now initialized **without** the backend argument. Now the LocalRuntime will default to `backend='ray'`, which allows passing through the `num_gpus` argument in the placement decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "forward-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local runtime collaborators = {'Portland': <openfl.experimental.interface.participants.Collaborator object at 0x7f60d474efd0>, 'Seattle': <openfl.experimental.interface.participants.Collaborator object at 0x7f60d474eee0>, 'Chandler': <openfl.experimental.interface.participants.Collaborator object at 0x7f60d474e220>, 'Bangalore': <openfl.experimental.interface.participants.Collaborator object at 0x7f60d474d1c0>}\n"
     ]
    }
   ],
   "source": [
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::len(collaborators)]\n",
    "    local_train.targets = mnist_train.targets[idx::len(collaborators)]\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "# The following is equivalent to\n",
    "# local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, **backend='ray'**, checkpoint=True)\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, checkpoint=True)\n",
    "print(f'Local runtime collaborators = {local_runtime._collaborators}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525eaa9",
   "metadata": {},
   "source": [
    "Now that we have our flow and runtime defined, let's run the experiment! \n",
    "\n",
    "(If you run this example on Google Colab with the GPU Runtime, you should see one task executing at a time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d581d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling start\n",
      "Performing initialization for model\n",
      "Sending state from aggregator to collaborators\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Performing aggregated model validation for collaborator Portland\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Performing aggregated model validation for collaborator Seattle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Test set: Avg. loss: 2.3169, Accuracy: 256/2500 (10%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Portland value of 0.10239999741315842\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.338894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.339085\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.295913\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Test set: Avg. loss: 2.3128, Accuracy: 267/2500 (11%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Seattle value of 0.10679999738931656\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.285198\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.321749\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.334687\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.281742\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.250912\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.305683\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.286804\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.291127\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.242928\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.310202\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.289258\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.262687\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.288993\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.280733\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 2.213716\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.235229\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 2.202511\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.261806\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.216602\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.161291\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 2.100578\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 2.244032\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 2.116627\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 2.146897\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.997094\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.083005\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.954156\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 2.115085\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.926068\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 2.014199\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.778255\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.991243\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.841314\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.505139\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.577045\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.778577\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 1.617106\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.542029\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 1.353919\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.373655\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.391970\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 1.367821\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 1.247690\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 1.095224\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 1.008592\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Test set: Avg. loss: 0.9136, Accuracy: 2034/2500 (81%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Doing local model validation for collaborator Portland: 0.8136000037193298\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242828)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Test set: Avg. loss: 0.8041, Accuracy: 2034/2500 (81%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Doing local model validation for collaborator Seattle: 0.8136000037193298\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242852)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Performing aggregated model validation for collaborator Bangalore\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Performing aggregated model validation for collaborator Chandler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m   warnings.warn(warning.format(ret))\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Test set: Avg. loss: 2.3114, Accuracy: 264/2500 (11%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Bangalore value of 0.10559999942779541\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.301883\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.343061\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Test set: Avg. loss: 2.3181, Accuracy: 245/2500 (10%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Chandler value of 0.09799999743700027\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.359025\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.310021\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 2.341894\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.264101\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.299526\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 2.304720\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.259850\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 2.287557\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.281858\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.283251\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.244680\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 2.299134\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.279016\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 2.296458\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 2.270685\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.250451\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.260360\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 2.237297\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 2.168439\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 2.224158\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.181003\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 2.230964\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 2.214376\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 2.044357\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.210622\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 1.964070\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 1.955216\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 2.153713\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 2.027297\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.853258\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 2.097017\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.835185\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.408818\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 2.033705\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.484282\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 1.846991\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 1.851501\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.159285\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 1.661282\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 1.215151\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 1.129316\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.673897\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.877755\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 1.554322\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 1.382948\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 1.313372\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Test set: Avg. loss: 0.7357, Accuracy: 2076/2500 (83%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Doing local model validation for collaborator Bangalore: 0.8303999900817871\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242993)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Test set: Avg. loss: 0.9787, Accuracy: 2030/2500 (81%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Doing local model validation for collaborator Chandler: 0.8119999766349792\n",
      "\u001b[2m\u001b[36m(wrapper pid=2242999)\u001b[0m Should transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "Average aggregated model validation values = 0.10319999791681767\n",
      "Average training loss = 1.1418851017951965\n",
      "Average local model validation values = 0.8173999935388565\n",
      "Sending state from aggregator to collaborators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Performing aggregated model validation for collaborator Portland\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Test set: Avg. loss: 0.8644, Accuracy: 2067/2500 (83%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Portland value of 0.8267999887466431\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Performing aggregated model validation for collaborator Seattle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.203063\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 1.252681\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Test set: Avg. loss: 0.8646, Accuracy: 2090/2500 (84%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Seattle value of 0.8359999656677246\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 1.167311\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.012679\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 1.065430\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.104719\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 1.098904\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.960898\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.989089\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.885600\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 1.231602\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.846674\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 1.221711\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.969600\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 1.049767\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 1.015426\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.784095\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.834525\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.976524\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.714737\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.690189\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.943430\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.728380\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.625124\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.643623\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 1.024824\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.687914\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.927897\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.777267\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.827286\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.510182\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.709723\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.910736\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.640569\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.476294\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.794358\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.697588\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.842378\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.683528\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.486341\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.912884\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.722120\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.881533\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.856469\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.527479\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.512443\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.596558\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.530014\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Test set: Avg. loss: 0.3362, Accuracy: 2272/2500 (91%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Doing local model validation for collaborator Portland: 0.9088000059127808\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243193)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Test set: Avg. loss: 0.3502, Accuracy: 2255/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Doing local model validation for collaborator Seattle: 0.9019999504089355\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243221)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Performing aggregated model validation for collaborator Bangalore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Performing aggregated model validation for collaborator Chandler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Test set: Avg. loss: 0.8499, Accuracy: 2133/2500 (85%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Bangalore value of 0.8531999588012695\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.340710\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.880815\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Test set: Avg. loss: 0.8733, Accuracy: 2092/2500 (84%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Chandler value of 0.8367999792098999\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.923796\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.416206\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 1.287715\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 1.499063\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.910390\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 1.227022\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.886411\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 1.046020\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.895784\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.940129\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.742270\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 1.086199\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.825261\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 1.020566\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.922339\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.760267\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.785324\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.893813\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.727813\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.919615\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.665999\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 1.062167\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.531452\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.673651\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.024402\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.555487\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.416934\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 1.031197\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.613623\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.873968\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.710868\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.876727\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.454118\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.942346\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.521891\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.537847\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.504537\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.645458\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.856629\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.666766\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.502265\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.986682\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.460076\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.789582\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.874171\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.680028\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Test set: Avg. loss: 0.3470, Accuracy: 2249/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Doing local model validation for collaborator Bangalore: 0.8995999693870544\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243332)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Test set: Avg. loss: 0.3806, Accuracy: 2239/2500 (90%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Doing local model validation for collaborator Chandler: 0.8955999612808228\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243334)\u001b[0m Should transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "Average aggregated model validation values = 0.8381999731063843\n",
      "Average training loss = 0.6316465809941292\n",
      "Average local model validation values = 0.9014999717473984\n",
      "Sending state from aggregator to collaborators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Performing aggregated model validation for collaborator Portland\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Test set: Avg. loss: 0.3232, Accuracy: 2285/2500 (91%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Portland value of 0.9139999747276306\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.666244\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.902902\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Performing aggregated model validation for collaborator Seattle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.588233\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.720431\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.708247\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Test set: Avg. loss: 0.3257, Accuracy: 2274/2500 (91%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Seattle value of 0.9095999598503113\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.686411\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.567578\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.519011\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.574776\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.559982\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.649355\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.500061\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.512979\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.563021\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.445138\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.498731\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.547046\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.633012\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.486236\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.966214\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.437983\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.766190\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.297839\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.561924\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.592235\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.444067\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.458562\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.515764\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.384137\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.560018\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.368921\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.697368\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.420167\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.496724\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.570038\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.608180\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.435898\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.442384\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.572413\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.380909\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.376018\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.765498\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.292984\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.360609\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.430709\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.280968\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Test set: Avg. loss: 0.2427, Accuracy: 2316/2500 (93%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Doing local model validation for collaborator Portland: 0.9264000058174133\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243469)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.505674\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.358114\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Test set: Avg. loss: 0.2622, Accuracy: 2307/2500 (92%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Doing local model validation for collaborator Seattle: 0.9228000044822693\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243501)\u001b[0m Should transfer from local_model_validation to join\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Performing aggregated model validation for collaborator Chandler\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Running aggregated_model_validation in a new process\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Calling aggregated_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Performing aggregated model validation for collaborator Bangalore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m /tmp/ipykernel_2237021/2166382283.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m /home/pfoley1/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m   warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Test set: Avg. loss: 0.3315, Accuracy: 2285/2500 (91%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Chandler value of 0.9139999747276306\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.694009\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.689014\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Test set: Avg. loss: 0.3139, Accuracy: 2292/2500 (92%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Bangalore value of 0.9167999625205994\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.688462\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Calling train\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.685028\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.731627\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [640/15000 (4%)]\tLoss: 0.502608\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.624344\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.777497\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [1280/15000 (9%)]\tLoss: 0.680268\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [1920/15000 (13%)]\tLoss: 0.594161\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.664482\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.581160\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.541899\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.629333\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.707280\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.859658\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [3840/15000 (26%)]\tLoss: 0.410364\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [4480/15000 (30%)]\tLoss: 0.464869\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.671656\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.514858\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.644807\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.687541\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [5760/15000 (38%)]\tLoss: 0.304619\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.342404\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.284546\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [7040/15000 (47%)]\tLoss: 0.425682\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.638228\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.343248\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.617751\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.905743\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [8320/15000 (55%)]\tLoss: 0.403404\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [8960/15000 (60%)]\tLoss: 0.339156\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.395792\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.477744\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.787099\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.402334\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.478597\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.626800\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [10880/15000 (72%)]\tLoss: 0.658170\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [11520/15000 (77%)]\tLoss: 0.396454\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.541565\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [12160/15000 (81%)]\tLoss: 0.808431\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.561275\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.276648\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.476980\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [13440/15000 (89%)]\tLoss: 0.414740\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [14080/15000 (94%)]\tLoss: 0.311170\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Train Epoch: 1 [14720/15000 (98%)]\tLoss: 0.391893\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Calling local_model_validation\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Test set: Avg. loss: 0.2505, Accuracy: 2316/2500 (93%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Doing local model validation for collaborator Chandler: 0.9264000058174133\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243600)\u001b[0m Should transfer from local_model_validation to join\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Test set: Avg. loss: 0.2546, Accuracy: 2305/2500 (92%)\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m \n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Doing local model validation for collaborator Bangalore: 0.921999990940094\n",
      "\u001b[2m\u001b[36m(wrapper pid=2243625)\u001b[0m Should transfer from local_model_validation to join\n",
      "\n",
      "Calling join\n",
      "Average aggregated model validation values = 0.913599967956543\n",
      "Average training loss = 0.49812114238739014\n",
      "Average local model validation values = 0.9244000017642975\n",
      "\n",
      "Calling end\n",
      "This is the end of the flow\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "flflow = CollaboratorGPUFlow(model,optimizer)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10616d60",
   "metadata": {},
   "source": [
    "Now that the flow has completed, let's get the final model and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ae3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the final model weights: tensor([[[ 0.2153,  0.0791,  0.2710,  0.3133,  0.2334],\n",
      "         [ 0.2520, -0.0510,  0.1686,  0.2465,  0.1977],\n",
      "         [-0.1622, -0.0522, -0.0641, -0.1615,  0.0453],\n",
      "         [-0.2252, -0.2087,  0.0602, -0.1192, -0.1359],\n",
      "         [-0.1748, -0.2163, -0.0687, -0.0904, -0.1098]]])\n",
      "\n",
      "Final aggregated model accuracy for 3 of training: 0.9108999669551849\n"
     ]
    }
   ],
   "source": [
    "print(f'Sample of the final model weights: {flflow.model.state_dict()[\"conv1.weight\"][0]}')\n",
    "\n",
    "print(f'\\nFinal aggregated model accuracy for {flflow.rounds} rounds of training: {flflow.aggregated_model_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e084b41",
   "metadata": {},
   "source": [
    "Now that the flow is complete, let's dig into some of the information captured along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = flflow._run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Metaflow, Flow, Task, Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Metaflow()\n",
    "list(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7d05f",
   "metadata": {},
   "source": [
    "For existing users of Metaflow, you'll notice this is the same way you would examine a flow after completion. Let's look at the latest run that generated some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Flow('CollaboratorGPUFlow').latest_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206b36c",
   "metadata": {},
   "source": [
    "And its list of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ec317",
   "metadata": {},
   "source": [
    "This matches the list of steps executed in the flow, so far so good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Step(f'CollaboratorGPUFlow/{run_id}/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5522b7",
   "metadata": {},
   "source": [
    "Now we see **12** steps: **4** collaborators each performed **3** rounds of model training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Task(f'FederatedFlow/{run_id}/train/9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5da76",
   "metadata": {},
   "source": [
    "Now let's look at the data artifacts this task generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data.input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92fab0",
   "metadata": {},
   "source": [
    "Now let's look at its log output (stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6e90e",
   "metadata": {},
   "source": [
    "And any error logs? (stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f8d25",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "Now that you've completed your **workflow interface 201 tutorial**, see some of the more advanced things you can do in our [other tutorials](broken_link), including:\n",
    "\n",
    "- Vertical Federated Learning\n",
    "- Model Watermarking\n",
    "- Differential Privacy\n",
    "- And More!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f6742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
